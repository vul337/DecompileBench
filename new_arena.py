# %%

import json5.parser
from langchain.globals import set_debug
from langchain import hub
from langchain.agents import AgentExecutor, create_react_agent, create_openai_functions_agent
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_community.llms import Tongyi
from langchain_openai import ChatOpenAI
from langchain.agents import AgentExecutor
from langchain.prompts import PromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate
from langchain_core.messages import SystemMessage, HumanMessage
from langchain_core.prompts.chat import ChatPromptTemplate, MessagesPlaceholder
import networkx as nx
from dotenv import load_dotenv
import concurrent.futures
from langchain_core.exceptions import OutputParserException
import matplotlib.pyplot as plt
import json
import numpy as np
import datasets
import asyncio
import ast
import pickle
from tqdm import tqdm
import json5
import math
from IPython.display import display
import plotly.express as px
from json import JSONDecodeError
import pandas as pd
from langchain.tools import BaseTool, StructuredTool, tool
import pathlib
from langchain_core.runnables import RunnablePassthrough
from langchain_core.runnables import RunnableLambda
import pathlib
import openai
import os
import pandas as pd
import itertools
import time
import requests
from typing_extensions import TypedDict
from typing import Annotated, List, Sequence, Tuple, Union, Set
from collections import defaultdict
from typing import List, Literal, Optional, Type, Dict, Any, Callable
import re
import operator
# from langchain_core.pydantic_v1 import BaseModel, Field
# from langchain.output_parsers.openai_tools import (
#     JsonOutputToolsParser,
#     PydanticToolsParser,
# )
import random
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.outputs import Generation
from langchain_core.utils.json import (
    parse_partial_json,
    _parse_json,
)


# %%

load_dotenv()

# %%

choice1 = '''\
As an experienced and professional reverse engineer, you possess the ability to evaluate code generated by two different decompilers in an objective and impartial manner. 
I will provide you with the source code that needs evaluation, followed by two separate decompiled versions of that code. 
Your task is to assess each decompiler's performance across various aspects. 
For some aspects, you will have three scoring options (-1, 0, 1), while for others, only two options (0, 1) will be available. The specific meanings of these scores can be found in the *Evaluation Choices* section.
When evaluating the decompiled code, you should compare it to the original source code, focusing primarily on the these 12 aspects. 

## **Readability**
1. **Typecast Issues**
    - Example:
        - Source code:
            
            ```c
            if (n < 1 || m < 1)
                error_exit("Invalid size");
            ```
            
        - Decompiled code:
            
            ```c
            if (n < 1 || m < 1)
                error_exit((long long) "Invalid size");
            ```   
        - **Explanation**: The decompiler introduces an unnecessary and incorrect `(long long)` cast, adding redundancy and confusing the reader. This additional cast serves no purpose and reduces the clarity of the code, making the code harder to read.
    - Evaluation Choices:
        - -1: The decompiled code contains an incorrect typecast.
        - 0: The decompiled code has an unnecessary typecast.
        - 1: The decompiled code does not contain any incorrect or unnecessary typecasts.
     
2. **Non-idiomatic Literal Representation**
    - Example:
        - Source code:
            ```c
            strcat(buffer, "}\\n");
            ``` 
        - Decompiled code:
            
            ```c
            *( (WORD *) (v3)) = 2685;
            ```
            
        - **Explanation**: Non-idiomatic representations of literals, such as turning `"\\\\n"` into `2685`, obscure the original meaning and make the logic harder to follow.
    - Evaluation Choices:
        - 0: The decompiled code has an non-idiomatic representation of literals.
        - 1: The decompiled code does not contain any non-idomatic representations of literals.

3. **Obfuscated Control Flow**
    - Example:
        - Source code:
            
            ```c
            while (pack->next_object != obj) {
                pack = pack->next_object;
            }
            ```
            
        - Decompiled code:
            
            ```c
            for(i=a2; a1 != (*((_QWORD *) (i + 64))); i = *((_QWORD *) (i + 64)));
            ```
            
        - **Explanation**: Overly complex pointer dereferencing in loops, such as `(*((_QWORD *) (i + 64)))`.  This complicates understanding what was originally a simple `while` loop, diminishes readability and makes it hard to reconstruct the original control flow.
    - Evaluation Choices:
        - 0: The decompiled code has an obfuscated control flow.
        - 1: The decompiled code does not contain any obfuscated control flow.

4. **Use of Decompiler-Specific Macros**
    - Example:
        - Decompiled code:
            
            ```c
            LOWWORD(v5)
            ```
            
        - **Explanation**: The introduction of decompiler-specific macros (e.g., `LOWWORD(v5)`) deviates from standard C, reducing the readability and portability of the decompiled code.
    - Evaluation Choices:
        - 0: The decompiled code has an use of decompiler-specific macros.
        - 1: The decompiled code does not contain any use of decompiler-specific macros.

5. **Incorrect Return Behavior**
    - Example:
        - Source code:
            
            ```c
            // No return statement
            ```
            
        - Decompiled code:
            
            ```c
            return _readfsqword(0x28u) ^ v3;
            ```    
        - **Explanation**: The inclusion of incorrect return statements, such as `return _readfsqword(0x28u) ^ v3`, introduces erroneous behavior that wasn’t part of the original logic.
    - Evaluation Choices:
        - 0: The decompiled code has an incorrect return behavior.
        - 1: The decompiled code does not contain any incorrect return behavior.


## **Helpfulness**

1. **Meaningless Identifier Names**
    - Example:
        - Source code:
            
            ```c
            return buffer;
            ```  
        - Decompiled code:
            
            ```c
            return v4;
            ```
        - **Explanation**: Generic identifier names like `v4` instead of meaningful names like `buffer` significantly reduce the helpfulness of the decompiled code.
    - Evaluation Choices:
        - 0: The decompiled code has meaningless identifier names, like `v4` instead of `buffer`.
        - 1: The decompiled code does not contain any semantic-wrong or meaningless identifier names.

2. **Incorrect Identifier Names**
    - Example:
        - Source code:
            
            ```c
            int total_count;
            ```
            
        - Decompiled code:
            
            ```c
            int error_flag;
            ```
    
        - **Explanation**:  Incorrect identifier names (e.g., `error_flag` instead of `total_count`) make the decompiled code misleading and harder to reason about.
    - Evaluation Choices:
        - -1: The decompiled code has incorrect identifier names, like `success_flag` instead of `total_count`.
        - 0: The decompiled code has confusing identifier names, like `number` instead of `total_count`.
        - 1: The decompiled code does not contain any incorrect or confusing identifier names.

3. **Expanded Symbols**
    - Example:
        - Source code:
            
            ```c
            sizeof(int *)
            ```
            
        - Decompiled code:
            
            ```c
            8
            ```
            
        - **Explanation**: Replacing `sizeof` expressions with hardcoded numbers like `8` can be misleading and makes the code less readable and less portable.
    - Evaluation Choices:
        - -1: The decompiled code has misleading symbols, like `0xFFFFFFFF` instead of `sizeof(int *)`.
        - 0: The decompiled code has expanded symbols, like `8` instead of `sizeof(int *)`.
        - 1: The decompiled code does not contain any expanded symbols.

4. **Overall Function Correctness**
    - **Explanation**: This aspect evaluates whether the decompiled code captures the core functionality of the original source code. For example, if the original source code implements an MD5 hashing function, the decompiled code should make this clear, even if the syntax or identifiers are somewhat altered. If the overall logic aligns but is hard to identify, it's less helpful.
    - Evaluation Choices:
        - -1: The decompiled code significantly deviates from the functionality of the original source code.
        - 0: The decompiled code captures some functionality.
        - 1: The decompiled code captures the core functionality of the original source code.

5. **Overall Functionality Precision**
    - **Explanation**: This goes a step beyond correctness, evaluating how clearly the decompiled code captures the *exact* functionality. If the original source code implements MD5 but the decompiled version suggests a different hash function (such as SHA-256), then the code is not considered precise. Being able to pinpoint specific implementations adds to the helpfulness.
    - Evaluation Choices:
        - 0: The decompiled code does not capture the exact functionality of the original source code.
        - 1: The decompiled code captures the exact functionality of the original source code.

## **Both**

1. **Non-Idiomatic Dereferencing**
    - **Example**:
        - Source code:
            
            ```c
            current->next = malloc(sizeof(Node));
            current = current->next;
            current->x = 0;
            current->y = 0;
            ```
            
        - Decompiled code:
            
            ```c
            *((_QWORD *)v5 + 8) = malloc(24LL);
            v5 = *((_QWORD *)(v5 + 8));
            *((_QWORD *)v5) = 0;
            ``` 
        - **Explanation**: The decompiled code uses cryptic pointer arithmetic and memor layout, such as `((_QWORD *)v5 + 8)`, instead of reflecting the natural usage of structured data and object dereferencing (`current->next`). This obscures the underlying logic of the code, reducing both its readability and helpfulness to a reverse engineer, who now has to decode not just the logic but also the data structure's layout.
    - Evaluation Choices:
        - 1: The decompiled code does not contain any non-idiomatic dereferencing.
        - 0: The decompiled code uses some pointer arithmetic and memory layout, such as `((_QWORD *)v5 + 8)`, instead of reflecting the natural usage of structured data and object dereferencing (`current->next`).

2. **Abuse of Memory Layout**
    - **Example**:
        - Decompiled code:
            
            ```c
            (*(void (__stdcall **)(int, _DWORD, _DWORD, _DWORD, _DWORD))(*(_DWORD *)lpD3DDevice_1 + 68))(
                                     lpD3DDevice_1,
                                     0,
                                     0,
                                     0,
                                     0);
            ```
            
        - **Explanation**: Here, the decompiled code doesn't recover the original function structure, resorting to manual dereferencing and extensive type casting. This leads to over-complicated explicit type manipulations, making it hard to identify what is being invoked without further investigation into the memory layout or the device object itself. 
    - Evaluation Choices:
        - 1: The decompiled code does not exhibit any abuse of memory layout, as it correctly reflects structured data usage and object dereferencing.
        - 0: The decompiled code demonstrates abuse of memory layout, as it relies on complex pointer arithmetic and manual dereferencing instead of using straightforward object-oriented access patterns.

You should consider the above points comprehensively, summarize and categorize them to evaluate different decompilers. **Think step by step** and output the evaluation results in a clear and structured way.
'''

# - -1: The decompiler has a problem in this aspect that can be misleading for reverse engineers, such as variable names that deviate from their original meaning or incorrect variable types.
# - 0: The decompiler has issues in this aspect that may introduce unnecessary redundant information, complex logic, or non-idiomatic expressions, but these do not have direct misleading effects.
# - 1: The decompiler does not have any issues in this aspect.
evaluation_prompt_1 = choice1 + '''\
**First**, evaluate the performance of each decompiler compared to the source code for each aspect and represent this with a score:
The scoring options for different aspects vary, so please refer to the evaluation choices above to score each aspect accordingly.

**Then**, give result of which decompiler is better according to the scores for each aspect.
**Finally**, you should output a json file to collect the scores of decompilers and the winner for every criterion, following the format below.

```json
{
  
    "Readability":{
    "Typecast Issues": {
         "A_score": -1,
         "B_score": 1,
         "winner": "B"
    },
    
    },
   "Helpfulness":{
   "Meaningless Identifier Names": {
         "A_score": 0,
         "B_score": 0,
         "winner": "Tie"
    },
   
   }
   "Both":{
   "Non-idiomatic dereference": {
         "A_score": 1,
         "B_score": -1,
         "winner": "A"
    },

   }
}
```
'''

# %%


def json5_loads(x, *args, **kwargs):
    try:
        return json5.loads(x)
    except ValueError as e:
        raise JSONDecodeError("Expecting value", x, 0) from e


json.loads = json5_loads
'''
from qwen_arena import enforce_prefix_parse_json_markdown
text="Evaluation of Decompiled Code/n#### 1. Incorrect Identifier Name/n- Decompiled Code A: The function name idna_to_unicode_44i is incorrect. The original function is idn_free./n- Decompiled Code B: The function name idn_free is/n"
enforce_prefix_parse_json_markdown(text)
'''

import argparse 
parser = argparse.ArgumentParser()
parser.add_argument('--run', action='store_true')
parser.add_argument('--rate', action='store_true')
args = parser.parse_args()

def enforce_prefix_parse_json_markdown(
    json_string: str, *args, parser: Callable[[str], Any] = parse_partial_json, require_prefix=True,
) -> dict:
    """
    Parse a JSON string from a Markdown string.

    Args:
        json_string: The Markdown string.

    Returns:
        The parsed JSON object as a Python dictionary.
    """

    def parse_json5_before(s: str, *, strict: bool = False):
        try:
            return json5.loads(s)
        except Exception:
            return parser(s)
    
    try:
        return _parse_json(json_string, parser=parser)
    except json.JSONDecodeError:
        # Try to find JSON string within triple backticks
        if require_prefix:
            pattern = r"```json(.*)```"
        else:
            pattern = r"```(.*)```"
        match = re.search(pattern, json_string, re.DOTALL)

        # If no match found, assume the entire string is a JSON string
        if match is None:
            json_str = json_string
        else:
            # If match found, use the content within the backticks
            json_str = match.group(1)

    try:
        return _parse_json(json_str, parser=parser)
    except json.JSONDecodeError as e:
        if require_prefix is True:
            return enforce_prefix_parse_json_markdown(json_string, *args, parser, require_prefix=False)
        else:
            raise e


class EnforcePrefixJsonOutputParser(JsonOutputParser):
    def parse_result(self, result: List[Generation], *, partial: bool = False) -> Any:
        text = result[0].text
        text = text.strip()
        if partial:
            try:
                return enforce_prefix_parse_json_markdown(text)
            except JSONDecodeError:
                return None
        else:
            try:
                return enforce_prefix_parse_json_markdown(text)
            except JSONDecodeError as e:
                msg = f"Invalid json output: {text}"
                raise OutputParserException(msg, llm_output=text) from e

# llm = ChatOpenAI(model="qwen2.5-72b-instruct", max_tokens=4096,   timeout=60 * 60, base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
#                         api_key="sk-264dcd6768264d8c869c430363349f8b")
llm = ChatOpenAI(model="Qwen/Qwen2.5-Coder-32B-Instruct",
max_tokens= 8192,timeout=60 * 60, base_url="https://platform.vul337.team:8443/v1",
    api_key="sk-vzp1lEbFHclEBEaw7234DcA04cA74d29BaB42625203915Dc")
json_output_parser = EnforcePrefixJsonOutputParser()
def parse_generation(text,partial: bool = False):
    text = text.strip()
    if partial:
        try:
            return enforce_prefix_parse_json_markdown(text)
        except JSONDecodeError:
            return None
    else:
        try:
            return enforce_prefix_parse_json_markdown(text)
        except JSONDecodeError as e:
            msg = f"Invalid json output: {text}"
            raise OutputParserException(msg, llm_output=text) from e
# %%
choice = 1

prompt = ChatPromptTemplate.from_messages([
    SystemMessage(content=evaluation_prompt_1),
    HumanMessagePromptTemplate.from_template(template='''\
Source Code:
{source_code}

Decompiled Code A:
{decompile_code_a}

Decompiled Code B:
{decompile_code_b}

''')
    ])  

chain = (prompt | llm | {
    "raw_output": RunnableLambda(lambda x: x.content),
    "parsed_output": json_output_parser,
})
# client = openai.AsyncClient(
#     base_url="https://platform.vul337.team:8443/v1",
#     api_key="sk-VyCFQlCUvscOH4tOEaDf955dD5Bd419c850e102a525c7044"
# )
decompilers = [ "angr", "binja", "dewolf", "ghidra",
    "hexrays", "mlm", "deepseek","qwen", "retdec",'llm4decompile','gpt-4o-mini','gpt-4o']
INIT_RATING=1000
aspects = [
        "Typecast Issues",
        "Non-idiomatic Literal Representation",
        "Obfuscated Control Flow",
        "Use of Decompiler-Specific Macros",
        "Incorrect Return Behavior",
        "Meaningless Identifier Names",
        "Incorrect Identifier Names",
        "Expanded Symbols",
        "Overall Function Correctness",
        "Overall Functionality Precision",
        "Non-Idiomatic Dereferencing",
        "Abuse of Memory Layout"
    ]
rating = {decompiler: INIT_RATING for decompiler in decompilers}
def format_message(message, role):
    return f"<s>{role}\n{message}</s>\n"

def prompt_format_decompile(src, dec_a, dec_b=None) -> str:
    # prompt = evaluation_prompt_double + f'Source Code: \n {src} \
    # Decompiled Code A:\n{dec_a} \
    # Decompiled Code B:\n{dec_b} \n <s>assistant\nassistant\n'
    # if dec_b is None:
    #     prompt_filled = prompt.format(source_code=src, decompile_code=dec_a)
    # else:
    #     prompt_filled = prompt.format(source_code=src, decompile_code_a=dec_a, decompile_code_b=dec_b)
    prompt_filled = prompt.format(source_code=src, decompile_code_a=dec_a, decompile_code_b=dec_b) 
    return prompt_filled

def invoke(src, dec_a, dec_b, metadata):
        # print(f"Invoking with {metadata}")
        # for i in range(3):
        for i in range(1):
            # print(f"invoking:{3}")
            try:
                ret = chain.invoke({
                    'source_code': src,
                    'decompile_code_a': dec_a,
                    'decompile_code_b': dec_b,
                })
                
                break
            except Exception as e:
                print(i, e)
                ret = None

        # print(f"ret: {ret.keys()}")

        return {
            'ret': ret,
            'metadata': metadata,
        }


def execute_from_generator(
        generator,
        fn,
        max_workers=8,
        parallel=False,
    ):
        futures = {}
        finished = False
        pbar = tqdm()
        ret_list = []
        with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers) as executor:
            while True:
                
                if futures:
                    done, not_done = concurrent.futures.wait(
                        futures, return_when=concurrent.futures.FIRST_COMPLETED)
                else:
                    done, not_done = set(), set()
                    not_done.add(None)
                    # done, not_done = concurrent.futures.wait(
                    #     futures, return_when=concurrent.futures.FIRST_COMPLETED)

                available_workers = max(
                    0, max_workers * 2 - len(not_done))
                print(f"available_workers: {available_workers}")
                # available_workers = 1
                for _ in range(available_workers):
                    try:
                        # print(_)
                        # import ipdb
                        # ipdb.set_trace()
                        task = next(generator)
                        print(_)
                        if parallel:
                            # print(f"submitting task:{task} ")
                            future = executor.submit(fn, **task)
                            futures[future] = task
                        else:
                            ret = fn(**task)
                            ret_list.append(ret)
                    except StopIteration:
                        finished = True
                        print("finished")
                        break
                
                print(done)
                for future in done:
                    print(f"future: {future}")
                    task = futures[future]
                    task_exception = future.exception()
                    pbar.update()

                    if task_exception is not None:
                        print(f'Task failed: {task_exception}')
                    else:
                        result = future.result()
                        # print(f"---Result: {result}")
                        yield result
                    del futures[future]

                pbar.set_description(
                    f"Remaining Tasks: {len(not_done)}, Finished: {finished}")#average 1min+ per item

                if len(not_done) == 0 and finished:
                    break

                if available_workers == 0 and len(done) == 0:
                    time.sleep(1)
        # return ret_list

  


def choose_another_dec(dec_a, decs):
    decs = decs.copy()
    decs.remove(dec_a)
    return np.random.choice(decs)

def choose_pair_by_elo(rating):
    """
    计算两个模型的 Elo 分数差异，并返回选择概率。
    分数差异越低，选择概率越高。
    """
    model_pairs = list(itertools.combinations(rating.keys(), 2))
    selection_probs = []
    
    for model1, model2 in model_pairs:
        elo_diff = abs(rating[model1] - rating[model2])
        selection_prob = np.exp(-elo_diff)
        selection_probs.append(selection_prob)
    
    # 归一化选择概率
    total_prob = sum(selection_probs)
    normalized_probs = [prob / total_prob for prob in selection_probs]
    
    # 根据选择概率随机选择一对模型
    selected_pair = np.random.choice(len(model_pairs), p=normalized_probs)
    
    return model_pairs[selected_pair]


def get_tasks_0(df: pd.DataFrame, rating):
    decompilers = list(rating.keys())
    
    dec_idx = 0
    for idx, row in df.iterrows():
        source_code = row['func']
    # for dw in decompilers:
        dec_a = decompilers[dec_idx%len(decompilers)]
        dec_idx += 1
        dec_b = choose_another_dec(dec_a, decompilers)
    # dec_b = choose_by_elo(rating)
        # print(f"a:{dec_a}, b:{dec_b}")
        dec_a_res = row[dec_a]
        dec_b_res = row[dec_b]

        if not dec_a_res or not dec_b_res or dec_a_res=='none' or dec_b_res=='none':
            continue

        yield {
            "src": source_code,
            "dec_a": dec_a_res,
            "dec_b": dec_b_res,
            "metadata": {
                "idx": idx,
                "a": dec_a,
                "b": dec_b,
            }
        }

def choose_by_elo(rating,model_a):
    """
    计算两个模型的 Elo 分数差异，并返回选择概率。
    分数差异越低，选择概率越高。
    """
    # model_pairs = list(itertools.combinations(rating.keys(), 2))
    # print(f"model_pairs: {model_pairs}")
    selection_probs = []
    rating_diff_list = []
    model_list = [model for model in rating.keys() if model != model_a]
    # max_score = max(rating.values())
    # for model,score in rating.items():
    #     rating[model] = score/max_score
    for model_b in model_list:
        rating_diff = abs(rating[model_a] - rating[model_b])
        rating_diff_list.append(rating_diff)
    rating_diff_min = min(rating_diff_list) + 1e-6
    for rating_diff in rating_diff_list:
        selection_prob = 1 / (1 + rating_diff/rating_diff_min)  # Inversely relate to the rating difference
        # selection_prob = np.exp(-rating_diff/rating_diff_min)  # Inversely relate to the rating difference
        selection_probs.append(selection_prob)
    
    # print(f"selection_probs: {selection_probs}")
    # 归一化选择概率
    total_prob = sum(selection_probs)
    normalized_probs = [prob / total_prob for prob in selection_probs]
    # exp_probs = np.exp(selection_probs)
    # total_prob = np.sum(exp_probs)
    # normalized_probs = exp_probs / total_prob
    # print(f"normalized_probs: {normalized_probs}")
    
    # 根据选择概率随机选择一对模型
    selected_b = np.random.choice(len(model_list), p=normalized_probs)
    
    return model_list[selected_b]


def get_tasks(df: pd.DataFrame, rating):
    decompilers = list(rating.keys())
    for idx, row in df.iterrows():
        # for dec_a in decompilers:
        dec_a = np.random.choice(decompilers)
        # dec_a = 'gpt-4o'
        source_code = row['func']
        dec_b = choose_by_elo(rating,dec_a)
        # import ipdb;ipdb.set_trace()
        # Randomly shuffle dec_a and dec_b
        if np.random.random() < 0.5:
            dec_a, dec_b = dec_b, dec_a
        
        dec_a_res = row[dec_a]
        dec_b_res = row[dec_b]

        if not dec_a_res or not dec_b_res or dec_a_res=='none' or dec_b_res=='none':
            continue

        yield {
            "src": source_code,
            "dec_a": dec_a_res,
            "dec_b": dec_b_res,
            "metadata": {
                "idx": idx,
                "a": dec_a,
                "b": dec_b,
            }
        }


def run(df: pd.DataFrame, rating,max_workers):
    result = []
    # import ipdb
    # ipdb.set_trace()
    print("="*15)
    for ret in execute_from_generator(
        get_tasks(df, rating),#dict_keys(['src', 'dec_a', 'dec_b', 'metadata'])
        invoke,
        max_workers=max_workers,
        parallel=True,
        ):
        # print(ret)
        result.append(ret)
    print(f"result: {len(result)}")
    return result





def main():
    save_dir = 'arena_general/all_chunks'
    if not pathlib.Path(save_dir).exists():
        pathlib.Path(save_dir).mkdir(parents=True)
    decompilers = [ 
        "angr", "binja", "dewolf", "ghidra",
        "hexrays", "mlm",  "retdec",
        'llm4decompile',
        'qwen',
        'deepseek',
        'gpt-4o-mini',
        'gpt-4o',
    ]


    # 获取当前目录中的所有 .pkl 文件
    pkl_files = [f for f in os.listdir(save_dir) if f.endswith('.pkl')]

    for pkl_file in pkl_files:
        try:
            # 尝试加载 .pkl 文件
            with open(os.path.join(save_dir,pkl_file), 'rb') as file:
                data = pickle.load(file)
            
            # 检查数据是否为空
            if len(data) == 0:
                print(f"Removing empty file: {pkl_file}")
                os.remove(os.path.join(save_dir,pkl_file))
                idx = int(pkl_file.split('_')[-1].split('.')[0])
                result_path = f'ratings_{idx}.json'
                if os.path.exists(os.path.join(save_dir,result_path)):
                    os.remove(os.path.join(save_dir,result_path))
        except Exception as e:
            print(f"Error loading {pkl_file}: {e}")

    def compute_online_elo(battles, calibrate_model, K=4, SCALE=400, BASE=10):
        
        for model_a, model_b,winner in battles:
            ra = rating[model_a]
            rb = rating[model_b]
            ea = 1 / (1 + BASE ** ((rb - ra) / SCALE))
            eb = 1 / (1 + BASE ** ((ra - rb) / SCALE))
            if winner == "a" or winner == "A":
                sa = 1
            elif winner == "b" or winner == "B":
                sa = 0
            elif winner == "Tie" or winner == "tie (bothbad)" or winner == "tie":
                sa = 0.5
            else:
                # raise Exception(f"unexpected vote {winner}")
                sa = 0.5
            # print(f"model_a: {model_a}, model_b: {model_b}, sa: {sa}, ea: {ea}, eb: {eb}")
            rating[model_a] += K * (sa - ea)
            rating[model_b] += K * (1 - sa - eb)

        # calibrate llama-13b to 800
        delta = (800-rating[calibrate_model])
        model_a_set = set([model_a for model_a, model_b,winner in battles])
        for model in model_a_set:
            rating[model] += delta

        return rating


    calibrate_model = 'ghidra'
    INIT_RATING=1000
    rating = defaultdict(lambda: INIT_RATING)
    for model in decompilers:
        rating[model] = INIT_RATING
    # needed_decompiler_ds = datasets.load_from_disk("/home/yuxincui/code/decompilebench-evaluation/result_merge_llm4")
    # needed_decompiler_ds = datasets.load_from_disk("/home/yuxincui/code/decompilebench-evaluation/decompileeval/output_dataset/ossfuzz_all_updated")
    needed_decompiler_ds = datasets.load_from_disk("/home/yuxincui/code/decompilebench-evaluation/decompileeval/output_dataset/ossfuzz_all_0109")
    
    df = needed_decompiler_ds.to_pandas()
    left_idx = 0
    right_idx = len(df)
    # df = df[left_idx:right_idx]

    result_list = os.listdir(save_dir)
    result_list = [result for result in result_list if result.startswith('ratings_')]
    result_list = sorted(result_list, key=lambda x: int(x.split('_')[-1].split('.')[0]))


    chunk_size = 100
    chunks = [df[i:i + chunk_size] for i in range(0, df.shape[0], chunk_size)]
    random.shuffle(chunks)
    chunk_idx = 0
    
    for chunk in chunks:
        print(f"rating: {rating}")
        
        with open(f'{save_dir}/ratings_{chunk_idx}.json', 'w') as file:
            json.dump(rating, file, indent=2)
        save_path = f"{save_dir}/chunk_{left_idx}_{chunk_size}_{chunk_idx}.pkl"
        chunk_idx += 1
        if os.path.exists(save_path):
            continue
        
        ret = run(chunk,rating,chunk_size//2)
       
        battles = []
        for _ret in ret:
            try:
                # import ipdb
                # ipdb.set_trace()
                eval_ret = _ret['ret']['parsed_output']
                print(f"eval_ret: {eval_ret}")
                idx = _ret['metadata']['idx']
                dec_a = _ret['metadata']['a']
                dec_b = _ret['metadata']['b']
                for k,v in eval_ret.items():
                    for kk,vv in v.items(): 
                        winner = vv['winner']
                        battles.append((dec_a, dec_b, winner))
            except Exception as e:
                print(f"no eval_ret: {e}")
                continue
             
        print(f"battles: {battles}")
        rating = compute_online_elo(battles, calibrate_model)
        print(f"chunk_idx: {chunk_idx}, rating: {rating}")

        with open(save_path, "wb") as f:
            pickle.dump(ret, f)

        
def compute_elo(save_dir='arena_general'):
    def get_bootstrap_result(battles, func_compute_elo, num_round):
        rows = []
        for i in tqdm(range(num_round), desc="bootstrap"):
            try:
                rows.append(func_compute_elo(
                    battles.sample(frac=1.0, replace=True)))
            except Exception as e:
                print(e)
        df = pd.DataFrame(rows)
        return df[df.median().sort_values(ascending=False).index]
    
    def compute_mle_elo(
        df, SCALE=800, BASE=10, INIT_RATING=1000
    ):
        from sklearn.linear_model import LogisticRegression
        ptbl_a_win = pd.pivot_table(
            df[df["winner"] == "model_a"],
            index="model_a",
            columns="model_b",
            aggfunc="size",
            fill_value=0,
        )
        # if no tie, create a zero matrix
        if sum(df["winner"].isin(["tie", "tie (bothbad)"])) == 0:
            ptbl_tie = pd.DataFrame(0, index=ptbl_a_win.index,
                                    columns=ptbl_a_win.columns)
        else:
            ptbl_tie = pd.pivot_table(
                df[df["winner"].isin(["tie", "tie (bothbad)"])],
                index="model_a",
                columns="model_b",
                aggfunc="size",
                fill_value=0,
            )
            ptbl_tie = ptbl_tie + ptbl_tie.T

        ptbl_b_win = pd.pivot_table(
            df[df["winner"] == "model_b"],
            index="model_a",
            columns="model_b",
            aggfunc="size",
            fill_value=0,
        )
        ptbl_win = ptbl_a_win * 2 + ptbl_b_win.T * 2 + ptbl_tie

        models = pd.Series(np.arange(len(ptbl_win.index)), index=ptbl_win.index)

        p = len(models)
        X = np.zeros([p * (p - 1) * 2, p])
        Y = np.zeros(p * (p - 1) * 2)

        cur_row = 0
        sample_weights = []
        for m_a in ptbl_win.index:
            for m_b in ptbl_win.columns:
                if m_a == m_b:
                    continue
                # if nan skip
                if math.isnan(ptbl_win.loc[m_a, m_b]) or math.isnan(ptbl_win.loc[m_b, m_a]):
                    continue
                X[cur_row, models[m_a]] = +math.log(BASE)
                X[cur_row, models[m_b]] = -math.log(BASE)
                Y[cur_row] = 1.0
                sample_weights.append(ptbl_win.loc[m_a, m_b])

                X[cur_row + 1, models[m_a]] = math.log(BASE)
                X[cur_row + 1, models[m_b]] = -math.log(BASE)
                Y[cur_row + 1] = 0.0
                sample_weights.append(ptbl_win.loc[m_b, m_a])
                cur_row += 2
        X = X[:cur_row]
        Y = Y[:cur_row]

        lr = LogisticRegression(fit_intercept=False, penalty=None, tol=1e-6)
        lr.fit(X, Y, sample_weight=sample_weights)
        elo_scores = SCALE * lr.coef_[0] + INIT_RATING
        if calibrate_model in models.index:
            elo_scores += 1141.86 - elo_scores[models[calibrate_model]]
        return pd.Series(elo_scores, index=models.index).sort_values(ascending=False)

    BOOTSTRAP_ROUNDS = 100  # 1000
    calibrate_model = 'ghidra'
    chunk_dir = f'{save_dir}/all_chunks'
    pkl_list = os.listdir(chunk_dir)
    df_list = []
    os.makedirs(f"{save_dir}/all_scores", exist_ok=True)
    print(f"pkl_list: {len(pkl_list)}")
    for pkl in pkl_list:
        if not pkl.endswith("pkl"):
            continue
        print(f"pkl: {pkl}")
        with open(f"{save_dir}/{pkl}", "rb") as f:
            ret = pickle.load(f)

        for battle in ret:
            metadata = battle['metadata']
            eval_ret = battle['ret']

            if not eval_ret:
                continue
            
            parsed_output = eval_ret['parsed_output']
            if not parsed_output:
                continue

            model_a = metadata['a']
            model_b = metadata['b']
            if type(parsed_output) is not dict:
                print(f"parsed_output: {parsed_output}")
                continue
            # import ipdb
            # ipdb.set_trace() 
               
            for k,v in parsed_output.items():
                try: 
                    for kk,vv in v.items(): 
                        if 'winner' not in vv:
                            print(vv.keys())
                            continue

                        winner_value = vv['winner']
                        if not winner_value:
                            winner = 'tie'
                        elif winner_value.lower() == 'a':
                            winner = 'model_a'
                        elif winner_value.lower() == 'b':
                            winner = 'model_b'
                        else:
                            winner = 'tie'
                        df_list.append({
                            'model_a': model_a,
                            'model_b': model_b,
                            "ds_idx": metadata['idx'],
                            'aspect': kk,
                            "winner": winner,
                        })
                except Exception as e:
                    print(f"parse output error: {e}")
                    continue

    print(f"df_list: {len(df_list)}")
    df = pd.DataFrame(df_list)
    # df['aspect_cleaned'] = df['aspect'].apply(
    #     lambda x: x.lower().replace(' ', ''))

    bootstrap_elo_lu = get_bootstrap_result(
        df, compute_mle_elo, BOOTSTRAP_ROUNDS)
    # import ipdb
    # ipdb.set_trace()
    bootstrap_elo_lu.to_csv(f"{save_dir}/all_scores/elo_scores.csv")
    global aspects
    for aspect in aspects:
        df_aspect = df[df['aspect'] == aspect]
        bootstrap_elo_lu_aspect =get_bootstrap_result(
            df_aspect, compute_mle_elo, BOOTSTRAP_ROUNDS)
        bootstrap_elo_lu_aspect.to_csv(f"{save_dir}/all_scores/elo_scores_{aspect}.csv")

def visualize(save_dir='arena_general'):
    # bootstrap_elo_lu = pd.read_csv(f"{save_dir}/elo_scores_{aspect}.csv")
    bootstrap_elo_lu = pd.read_csv(f"{save_dir}/all_scores/elo_scores.csv")
    #  'DataFrame' object has no attribute 'remove_prefix'
    # bootstrap_elo_lu = bootstrap_elo_lu.remove_prefix("Unnamed: 0")
    bootstrap_elo_lu = bootstrap_elo_lu.drop(columns=["Unnamed: 0"])
    def visualize_bootstrap_scores(df, title):
        bars = pd.DataFrame(dict(
            lower=df.quantile(.025),
            rating=df.quantile(.5),
            upper=df.quantile(.975))).reset_index(names="model").sort_values("rating", ascending=False)
        bars['error_y'] = bars['upper'] - bars["rating"]
        bars['error_y_minus'] = bars['rating'] - bars["lower"]
        bars['rating_rounded'] = np.round(bars['rating'], 2)
        fig = px.scatter(bars, x="model", y="rating", error_y="error_y",
                        error_y_minus="error_y_minus", text="rating_rounded",
                        title=title)
        fig.update_layout(xaxis_title="Model", yaxis_title="Rating",
                        height=600)
        return fig


    fig = visualize_bootstrap_scores(
        bootstrap_elo_lu, "Bootstrap of MLE Elo Rating Estimates")
    # display(fig)
    # 'Figure' object has no attribute 'save'
    # fig.save("arena/elo_scores.html")
    fig.write_html(f"{save_dir}/all_scores/elo_scores.html")

def compute_aspect_scores(save_dir='arena_general'):
    """Compute and save scores for each model across different aspects"""
    
    global aspects

    model_scores = {}
    
    for aspect in aspects:
        try:
            # Read ELO scores for this aspect
            df = pd.read_csv(f"{save_dir}/all_scores/elo_scores_{aspect}.csv")
            df = df.drop(columns=["Unnamed: 0"])
            
            # Get median scores for each model
            median_scores = df.median()
            
            # Store scores for each model
            for model in median_scores.index:
                if model not in model_scores:
                    model_scores[model] = {}
                model_scores[model][aspect] = median_scores[model]
                
        except FileNotFoundError:
            print(f"No scores found for aspect: {aspect}")
            continue
    
    # Convert to DataFrame for easier analysis
    scores_df = pd.DataFrame(model_scores).T
    
    # Save to CSV
    scores_df.to_csv(f"{save_dir}/all_scores/model_aspect_scores.csv")
    
    # Also save as JSON for easier parsing
    scores_dict = scores_df.to_dict('index')
    with open(f"{save_dir}/all_scores/model_aspect_scores.json", 'w') as f:
        json.dump(scores_dict, f, indent=2)
        
    return scores_df

def save_scores(save_dir='arena_general'):
    # global aspects
    for aspect in aspects:
        compute_elo(save_dir)
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)
    compute_aspect_scores(save_dir)


if __name__ == "__main__":
    if args.run:
        asyncio.run(main())
    if args.rate:
        save_scores('arena_general/')
    # if args.visualize:
    #     visualize('arena_general/all_chunks')
# %%
# import pickle
# path="/home/yuxincui/code/decompilebench-evaluation/decompileeval/arena_general/chunk_0_100_1.pkl"
# with open(path, "rb") as f:
#     ret = pickle.load(f)
# a={'Readability': {'Typecast Issues': {'A_score': -1, 'B_score': -1, 'winner': 'Tie'}, 'Non-idiomatic Literal Representation': {'A_score': -1, 'B_score': -1, 'winner': 'Tie'}, 'Obfuscated Control Flow': {'A_score': -1, 'B_score': -1, 'winner': 'Tie'}, 'Use of Decompiler-Specific Macros': {'A_score': 0, 'B_score': 0, 'winner': 'Tie'}, 'Incorrect Return Behavior': {'A_score': -1, 'B_score': -1, 'winner': 'Tie'}}, 'Helpfulness': {'Meaningless Identifier Names': {'A_score': -1, 'B_score': -1, 'winner': 'Tie'}, 'Incorrect Identifier Names': {'A_score': 0, 'B_score': 0, 'winner': 'Tie'}, 'Expanded Symbols': {'A_score': -1, 'B_score': -1, 'winner': 'Tie'}, 'Overall Function Correctness': {'A_score': 0, 'B_score': 0, 'winner': 'Tie'}, 'Overall Functionality Precision': {'A_score': -1, 'B_score': -1, 'winner': 'Tie'}}, 'Both': {'Non-idiomatic dereference': {'A_score': -1, 'B_score': -1, 'winner': 'Tie'}, 'Abuse of Memory Layout': {'A_score': -1, 'B_score': -1, 'winner': 'Tie'}}}
# %%
